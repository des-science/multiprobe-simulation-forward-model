{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ac3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da56f07-95b1-433f-b1f9-8ffee62e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from icecream import ic\n",
    "\n",
    "from msfm.utils import analysis, parameters\n",
    "from msfm import fiducial_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d37123",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b15243f-9854-4321-854d-e71a98f57e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-01 05:34:53  analysis.py INF   Loaded the config \n"
     ]
    }
   ],
   "source": [
    "conf = analysis.load_config()\n",
    "\n",
    "# tfr_pattern = \"/cluster/scratch/athomsen/CosmoGrid/v1/fiducial/DESy3_fiducial_???.tfrecord\"\n",
    "# tfr_pattern = \"/Users/arne/data/DESY3/tfrecords/v2/DESy3_fiducial_000.tfrecord\"\n",
    "tfr_pattern = \"/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_???.tfrecord\"\n",
    "\n",
    "params = [\"Om\", \"s8\", \"Aia\"]\n",
    "pert_labels = parameters.get_fiducial_perturbation_labels(params)\n",
    "n_perts = len(pert_labels)\n",
    "\n",
    "n_z_bins = 4\n",
    "global_batch_size = 16\n",
    "examples_shuffle_buffer = 10\n",
    "n_readers = 1\n",
    "n_prefetch = 3\n",
    "n_batches = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b09f37",
   "metadata": {},
   "source": [
    "### strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bdcec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 05:34:54.830821: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 05:34:56.547844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78993 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2023-03-01 05:34:56.549043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78993 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2023-03-01 05:34:56.550049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78993 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2023-03-01 05:34:56.551052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78993 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "# strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])\n",
    "# strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "print(strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9d535-57b9-4a37-88f0-ee9d8768ee01",
   "metadata": {},
   "source": [
    "# single noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5132479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-01 05:34:57 fiducial_pip INF   Starting to generate the fiducial training set for i_noise = 0 \n",
      "23-03-01 05:34:57  analysis.py INF   Loaded the config \n",
      "23-03-01 05:34:57  analysis.py INF   Loaded the pixel file \n",
      "23-03-01 05:34:57  analysis.py INF   Loaded the config \n",
      "23-03-01 05:34:57  analysis.py INF   Loaded the pixel file \n",
      "0\n",
      "23-03-01 05:34:58 fiducial_pip INF   Sharding the dataset over 4 replicas \n",
      "23-03-01 05:34:58 fiducial_pip INF   Using a local batch size of 4 per replica \n",
      "\u001b[1m\u001b[93m23-03-01 05:34:58 tfrecords.py WAR   Tracing parse_inverse_fiducial \u001b[0m\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x145cddfc1ee0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x145cddfc1ee0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "23-03-01 05:34:59  analysis.py INF   Loaded the config \n",
      "\u001b[1m\u001b[93m23-03-01 05:34:59 fiducial_pip WAR   Tracing dset_add_bias \u001b[0m\n",
      "\u001b[1m\u001b[93m23-03-01 05:34:59 fiducial_pip WAR   Tracing dset_add_noise \u001b[0m\n",
      "\u001b[1m\u001b[93m23-03-01 05:34:59 fiducial_pip WAR   Tracing dset_concat_perts \u001b[0m\n",
      "23-03-01 05:34:59 fiducial_pip INF   Taking 7 batches from the dataset \n",
      "23-03-01 05:34:59 fiducial_pip INF   Successfully generated the fiducial training set with element_spec (TensorSpec(shape=(28, 463872, 4), dtype=tf.float32, name=None), (TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(4,), dtype=tf.int32, name=None))) for i_noise = 0 \n"
     ]
    }
   ],
   "source": [
    "def dataset_fn(input_context):\n",
    "    dset = fiducial_pipeline.get_fiducial_dset(\n",
    "        tfr_pattern,\n",
    "        pert_labels,\n",
    "        global_batch_size,\n",
    "        conf=None,\n",
    "        n_batches=n_batches,\n",
    "        i_noise=0,\n",
    "        n_readers=n_readers,\n",
    "        n_prefetch=n_prefetch,\n",
    "        examples_shuffle_buffer=examples_shuffle_buffer,\n",
    "        input_context=input_context\n",
    "    )\n",
    "\n",
    "    return dset\n",
    "\n",
    "dist_dset = strategy.distribute_datasets_from_function(dataset_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84e0b24-ed0c-4078-a334-4c345f832aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([587, 453, 373, 513])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([224, 221, 571, 710])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([147,  95, 185,  78])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([306, 516, 179, 283])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n",
      "1\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([440, 361, 611,  45])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([312, 183, 538, 370])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 46, 604, 152, 326])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([556,  62, 672, 705])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n",
      "2\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  8, 399, 749, 568])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([429, 242, 120, 750])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([164, 791, 396,  65])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([509, 173, 308, 363])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n",
      "3\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([653, 562, 709, 658])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([110, 746, 449, 435])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 74, 443, 782, 550])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([580,  87,   2, 275])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n",
      "4\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([742, 463, 481, 230])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([483, 234, 380, 125])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([630, 404,  41,  35])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([142, 193, 482, 794])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n",
      "5\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([588, 356, 762,  24])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 54,  23, 737, 751])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([329, 216, 614, 452])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([421,  65, 203, 562])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n",
      "6\n",
      "(PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 25,  87, 509, 343])>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([270,  41, 110, 709])>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([382, 687, 449, 746])>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int64, numpy=array([483, 404, 463, 653])>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  1: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  2: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>,\n",
      "  3: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data_vectors, index in dist_dset:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    \n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e1b41b-fca1-4bdd-9ea3-3ebb8c02158f",
   "metadata": {},
   "source": [
    "# multi noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14223fe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m      2\u001b[0m     dset \u001b[38;5;241m=\u001b[39m fiducial_pipeline\u001b[38;5;241m.\u001b[39mget_fiducial_multi_noise_dset(\n\u001b[1;32m      3\u001b[0m         tfr_pattern,\n\u001b[1;32m      4\u001b[0m         pert_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         input_context\u001b[38;5;241m=\u001b[39minput_context\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dset\n\u001b[0;32m---> 17\u001b[0m dist_dset \u001b[38;5;241m=\u001b[39m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_datasets_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_vectors, index \u001b[38;5;129;01min\u001b[39;00m dist_dset:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data_vectors\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/global/common/software/des/athomsen/deep_lss/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1186\u001b[0m, in \u001b[0;36mStrategyBase.distribute_datasets_from_function\u001b[0;34m(self, dataset_fn, options)\u001b[0m\n\u001b[1;32m   1182\u001b[0m distribution_strategy_input_api_counter\u001b[38;5;241m.\u001b[39mget_cell(\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribute_datasets_from_function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mincrease_by(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m-> 1186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribute_datasets_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/des/athomsen/deep_lss/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_strategy.py:593\u001b[0m, in \u001b[0;36mMirroredExtended._distribute_datasets_from_function\u001b[0;34m(self, dataset_fn, options)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_workers):\n\u001b[1;32m    588\u001b[0m   input_contexts\u001b[38;5;241m.\u001b[39mappend(distribute_lib\u001b[38;5;241m.\u001b[39mInputContext(\n\u001b[1;32m    589\u001b[0m       num_input_pipelines\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[1;32m    590\u001b[0m       input_pipeline_id\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    591\u001b[0m       num_replicas_in_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_replicas_in_sync))\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minput_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distributed_datasets_from_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_container_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/des/athomsen/deep_lss/lib/python3.9/site-packages/tensorflow/python/distribute/input_util.py:132\u001b[0m, in \u001b[0;36mget_distributed_datasets_from_function\u001b[0;34m(dataset_fn, input_workers, input_contexts, strategy, options, build)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_place_dataset_on_device` can not be set to True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen experimental_fetch_to_device is True and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplication mode is set to `PER_REPLICA`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf2\u001b[38;5;241m.\u001b[39menabled():\n\u001b[0;32m--> 132\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minput_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDistributedDatasetsFromFunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_contexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_contexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataset_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbuild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m input_lib_v1\u001b[38;5;241m.\u001b[39mDistributedDatasetsFromFunctionV1(\n\u001b[1;32m    142\u001b[0m       input_workers, strategy, input_contexts, dataset_fn, options)\n",
      "File \u001b[0;32m/global/common/software/des/athomsen/deep_lss/lib/python3.9/site-packages/tensorflow/python/distribute/input_lib.py:1372\u001b[0m, in \u001b[0;36mDistributedDatasetsFromFunction.__init__\u001b[0;34m(self, input_workers, strategy, input_contexts, dataset_fn, options, components, element_spec, build)\u001b[0m\n\u001b[1;32m   1370\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_built \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m build:\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/global/common/software/des/athomsen/deep_lss/lib/python3.9/site-packages/tensorflow/python/distribute/input_lib.py:1393\u001b[0m, in \u001b[0;36mDistributedDatasetsFromFunction.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_built\n\u001b[1;32m   1391\u001b[0m distribute_start_time_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets, element_spec \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1393\u001b[0m     \u001b[43m_create_datasets_from_function_with_input_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   1396\u001b[0m   \u001b[38;5;66;03m# Records the time to initialize the distributed dataset.\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m   context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/global/common/software/des/athomsen/deep_lss/lib/python3.9/site-packages/tensorflow/python/distribute/input_lib.py:1875\u001b[0m, in \u001b[0;36m_create_datasets_from_function_with_input_context\u001b[0;34m(input_contexts, input_workers, dataset_fn)\u001b[0m\n\u001b[1;32m   1873\u001b[0m   worker \u001b[38;5;241m=\u001b[39m input_workers\u001b[38;5;241m.\u001b[39mworker_devices[i]\n\u001b[1;32m   1874\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(worker):\n\u001b[0;32m-> 1875\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mappend(dataset)\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets, dataset\u001b[38;5;241m.\u001b[39melement_spec\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mdataset_fn\u001b[0;34m(input_context)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataset_fn\u001b[39m(input_context):\n\u001b[1;32m      2\u001b[0m     dset \u001b[38;5;241m=\u001b[39m fiducial_pipeline\u001b[38;5;241m.\u001b[39mget_fiducial_multi_noise_dset(\n\u001b[1;32m      3\u001b[0m         tfr_pattern,\n\u001b[1;32m      4\u001b[0m         pert_labels,\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mbatch_size\u001b[49m,\n\u001b[1;32m      6\u001b[0m         conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         n_batches\u001b[38;5;241m=\u001b[39mn_batches,\n\u001b[1;32m      8\u001b[0m         n_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      9\u001b[0m         n_readers\u001b[38;5;241m=\u001b[39mn_readers,\n\u001b[1;32m     10\u001b[0m         n_prefetch\u001b[38;5;241m=\u001b[39mn_prefetch,\n\u001b[1;32m     11\u001b[0m         examples_shuffle_buffer\u001b[38;5;241m=\u001b[39mexamples_shuffle_buffer,\n\u001b[1;32m     12\u001b[0m         input_context\u001b[38;5;241m=\u001b[39minput_context\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "def dataset_fn(input_context):\n",
    "    dset = fiducial_pipeline.get_fiducial_multi_noise_dset(\n",
    "        tfr_pattern,\n",
    "        pert_labels,\n",
    "        batch_size,\n",
    "        conf=None,\n",
    "        n_batches=n_batches,\n",
    "        n_noise=3,\n",
    "        n_readers=n_readers,\n",
    "        n_prefetch=n_prefetch,\n",
    "        examples_shuffle_buffer=examples_shuffle_buffer,\n",
    "        input_context=input_context\n",
    "    )\n",
    "\n",
    "    return dset\n",
    "\n",
    "dist_dset = strategy.distribute_datasets_from_function(dataset_fn)\n",
    "\n",
    "for data_vectors, index in dist_dset:\n",
    "    print(data_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcabf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_lss",
   "language": "python",
   "name": "deep_lss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a491ce236b7462c2d10c5eba11df8cbf31130dea204ba4d12248d0f0c801e5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
