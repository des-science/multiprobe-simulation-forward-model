# Copyright (C) 2022 ETH Zurich, Institute for Particle Physics and Astrophysics

"""
Created December 2022
Author: Arne Thomsen

Closely follows
https://cosmo-gitlab.phys.ethz.ch/jafluri/cosmogrid_kids1000/-/blob/master/kids1000_analysis/base_model.py
by Janis Fluri, 
the main difference is that I don't work with horovod and instead use the builtin distribution strategies of
TensorFlow.
"""

import tensorflow as tf
import os, warnings

from msfm.utils import logger
from msfm.models.estimators import estimator_1st_order

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("once", category=UserWarning)
LOGGER = logger.get_logger(__file__)


class BaseModel(object):
    """
    This is a base model that provides a minimal training step and methods to restore and save the model.
    """

    def __init__(
        self,
        network,
        input_shape=None,
        optimizer=None,
        dir_summary=None,
        dir_checkpoint=None,
        restore_from_checkpoint=False,
        max_checkpoints=3,
        init_step=0,
    ):
        """Initializes a base model

        Args:
            network (tf.keras.Sequential): The underlying network of the model
            input_shape (tf.tensor, optional): Input shape of the network, necessary if one wants to restore the
                model. Defaults to None.
            optimizer (tf.keras.optimizers.Optimizer, optional): Optimizer of the model. Defaults to None.
            dir_summary (str, optional): Directory to save the summaries. Defaults to None.
            dir_checkpoint (str, optional): Directory where to save the weights and optimizer. Defaults to None.
            restore_checkpoint (boo, optional): Whether to restore the network from a checkpoint, or initialize it.
                Defaults to False.
            max_checkpoints (int, optional): The maximum number of checkpoints to keep. Older ones are automatically
                deleted by the CheckpointManager.
            init_step (int, optional): Initial step. Defaults to 0.
        """

        # get the network
        self.network = network

        # save additional variables
        self.input_shape = input_shape
        self.dir_summary = dir_summary
        self.dir_checkpoint = dir_checkpoint
        self.restore_from_checkpoint = restore_from_checkpoint
        self.init_step = init_step

        # set up the optimizer
        if optimizer is None:
            self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
        else:
            self.optimizer = optimizer

        # build the network
        if self.input_shape is not None:
            self.build_network(input_shape=self.input_shape)
            self.print_summary()

        # set the step
        self.train_step = tf.Variable(self.init_step, trainable=False, name="GlobalStep", dtype=tf.int64)
        tf.summary.experimental.set_step(self.train_step)

        # set up the checkpointing
        if self.dir_checkpoint is not None:
            os.makedirs(self.dir_checkpoint, exist_ok=True)
            self.checkpoint = tf.train.Checkpoint(network=self.network, optimizer=self.optimizer, train_step=self.train_step)
            self.checkpoint_manager = tf.train.CheckpointManager(
                self.checkpoint,
                self.dir_checkpoint,
                max_to_keep=max_checkpoints,
                checkpoint_name="ckpt",
                step_counter=self.train_step,
            )
        else:
            self.checkpoint = None
            self.checkpoint_manager = None

        if self.restore_from_checkpoint:
            self.restore_model()
        elif len(self.checkpoint_manager.checkpoints) != 0:
            LOGGER.warning(
                f"The model can not be saved when it is initialized from scratch with a non-empty checkpoint directory"
            )

        # set up summary writer
        if self.dir_summary is not None:
            os.makedirs(self.dir_summary, exist_ok=True)
            self.summary_writer = tf.summary.create_file_writer(dir_summary)
        else:
            self.summary_writer = None

        # estimator TODO
        # self.estimator = None

    def update_step(self):
        """
        Increments the train step of the model by 1
        """
        self.train_step.assign(self.train_step + 1)

    def set_step(self, step):
        """Sets the current training step of the model to a given value

        Args:
            step (int): The new step
        """
        self.train_step.assign(step)

    def save_model(self):
        """Saves the model with the CheckpointManager

        Raises:
            ValueError: If there's no checkpoint directory.
            Exception: When the model is initialized from scratch, but the given checkpoint directory is non-empty.
        """
        if self.dir_checkpoint is None:
            raise ValueError("No checkpoint directory was declared during the init of the model, it can not be saved.")

        else:
            if not self.restore_from_checkpoint and len(self.checkpoint_manager.checkpoints) != 0:
                raise Exception(
                    f"The specified checkpoint directory {self.dir_checkpoint} is not empty, can not save a model"
                    f" initialized from scratch there."
                )
            else:
                self.checkpoint_manager.save()
                LOGGER.info(f"Successfully saved the model to {self.checkpoint_manager.directory}")

    def restore_model(self):
        """Restores the model from a checkpoint using the CheckpointManager that picks the most recent checkpoint.

        Raises:
            ValueError: If there's no checkpoint directory or it's empty.
        """
        if self.dir_checkpoint is not None:
            dir_restore = self.checkpoint_manager.restore_or_initialize()
            LOGGER.info(f"Network successfully restored from checkpoint {dir_restore}.")

        else:
            raise ValueError(f"No checkpoint directory was given, the network can not be restored.")

    def build_network(self, input_shape):
        """Builds the internal HealpyGCNN with a given input shape

        Args:
            input_shape (tuple): Input shape of the netork, may contain None (like for the batch dimension)
        """
        self.network.build(input_shape=input_shape)

    def print_summary(self, **kwargs):
        """Prints the summary of the internal network

        Args:
            kwargs: passed to HealpyGCNN.summary
        """
        self.network.summary(**kwargs)

    def base_train_step(
        self,
        input_tensor,
        loss_function,
        input_labels=None,
        clip_by_value=None,
        clip_by_norm=None,
        clip_by_global_norm=None,
        l2_norm_weight=None,
    ):
        """A base train step given a loss funtion and an input tensor. The method evaluates the network and performs a
        singleg adient decent step. Note it should be wrapped in a tf.function. If multiple clippings are requested,
        the order will be:
            * by value
            * by norm
            * by global norm

        Args:
            input_tensor (tf.tensor): The input to the network
            loss_function (callable): The loss function, a callable that takes predictions of the network (and if
                provided, the input_labels) as input and returns a loss
            input_labels (tf.tensor, optional): Labels of the input_tensor. Defaults to None.
            clip_by_value (tf.tensor, optional): Clip the gradients by given 1d array of values into the interval
                [value[0], value[1]]. Defaults to None (no clipping).
            clip_by_norm (tf.tensor, optional): Clip the gradients by norm. Defaults to None (no clipping).
            clip_by_global_norm (tf.tensor, optional): Clip the gradients by global norm. Defaults to None (no
                clipping).
            l2_norm_weight (float, optional): Weight for the L2 norm of the trainable weights. Defaults to None
                (no regularization).
        """
        LOGGER.warning("Performing a base_train_step in python instead of a tf.function")
        trainable_weights = self.network.trainable_weights

        with tf.GradientTape() as tape:
            predictions = self.network(input_tensor, training=True)
            # compute the loss
            if input_labels is None:
                loss_val = loss_function(predictions)
            else:
                loss_val = loss_function(predictions, input_labels)
            if self.summary_writer is not None:
                with self.summary_writer.as_default():
                    tf.summary.scalar("loss", loss_val)

            # handle the l2 norm
            if l2_norm_weight is not None:
                l2_loss = tf.linalg.global_norm(trainable_weights)
                if self.summary_writer is not None:
                    with self.summary_writer.as_default():
                        tf.summary.scalar("l2_loss", l2_loss)
                loss_val = loss_val + l2_norm_weight * l2_loss

        # get the gradients
        gradients = tape.gradient(loss_val, trainable_weights)

        # clip the gradients
        if clip_by_value is not None:
            gradients = [tf.clip_by_value(g, clip_by_value[0], clip_by_value[1]) for g in gradients]
        if clip_by_norm is not None:
            gradients = [tf.clip_by_norm(g, clip_by_norm) for g in gradients]

        glob_norm = tf.linalg.global_norm(gradients)
        if self.summary_writer is not None:
            with self.summary_writer.as_default():
                tf.summary.scalar("global_grad_norm", glob_norm)

        if clip_by_global_norm is not None:
            gradients, _ = tf.clip_by_global_norm(gradients, clip_by_global_norm, use_norm=glob_norm)

        # apply gradients
        self.optimizer.apply_gradients(zip(gradients, trainable_weights))

        # update the step
        self.update_step()

    # # TODO: set up logic to save and restore estimators
    # def setup_1st_order_estimator(
    #     self,
    #     dset,
    #     fidu_param,
    #     off_sets,
    #     print_params=True,
    #     tf_dtype=tf.float32,
    #     tikohnov=0.0,
    #     layer=None,
    #     dset_is_sims=False,
    # ):
    #     """
    #     Sets up a first order estimator from a given dataset that will be evaluated
    #     :param dset: The dataset that will be evaluated
    #     :param fidu_param: the fiducial parameter of the estimator
    #     :param off_sets: the offsets used for the perturbations
    #     :param print_params: print the calculated params
    #     :param tf_dtype: the tensorflow datatype to use
    #     :param tikohnov: Add tikohnov regularization before inverting the jacobian
    #     :param layer: integer, propagate only up to this layer, can be -1
    #     :param dset_is_sims: If Ture, dset will be treated as evaluations
    #     """
    #     # set the layer
    #     self.estimator_layer = layer

    #     # dimension check
    #     fidu_param = np.atleast_2d(fidu_param)
    #     n_param = fidu_param.shape[-1]
    #     n_splits = 2 * n_param + 1

    #     if dset_is_sims:
    #         predictions = dset
    #     else:
    #         # get the predictions
    #         predictions = []
    #         for batch in dset:
    #             predictions.append(
    #                 np.split(
    #                     self.__call__(batch, training=False, layer=self.estimator_layer).numpy(),
    #                     indices_or_sections=n_splits,
    #                     axis=0,
    #                 )
    #             )
    #         # concat
    #         predictions = np.concatenate(predictions, axis=1)

    #     self.estimator = estimator_1st_order(
    #         sims=predictions,
    #         fiducial_point=fidu_param,
    #         offsets=off_sets,
    #         print_params=print_params,
    #         tf_dtype=tf_dtype,
    #         tikohnov=tikohnov,
    #     )

    # def estimate(self, input_tensor):
    #     """
    #     Calculates the first order estimates of the underlying model parameter given a network input
    #     :param input_tensor: The input to feed in the network
    #     :return: The parameter estimates
    #     """

    #     if self.estimator is None:
    #         raise ValueError("First order estimator not set! Call <setup_1st_order_estimator> first!")

    #     preds = self.__call__(input_tensor, training=False, layer=self.estimator_layer)
    #     return self.estimator(preds)

    def __call__(self, input_tensor, training=False, numpy=False, layer=None, *args, **kwargs):
        """Calls the network underlying the model

        Args:
            input_tensor (tf.tensor, np.ndarray): the tensor (or array) to call on
            training (bool, optional): Whether we are training or evaluating (e.g. necessary for batch norm). Defaults
                to False.
            numpy (bool, optional): Return a numpy array instead of a tensor. Defaults to False.
            layer (int, optional): Propagate only up to this layer, can be -1. Defaults to None.

        Returns:
            tf.tensor, np.ndarray: Tensor or array, depending on the numpy argument
        """
        if layer is None:
            preds = self.network(input_tensor, training=training, *args, **kwargs)
        else:
            preds = input_tensor
            for layer in self.network.layers[:layer]:
                preds = layer(preds)

        if numpy:
            return preds.numpy()
        else:
            return preds
