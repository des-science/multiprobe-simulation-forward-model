{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ac3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da56f07-95b1-433f-b1f9-8ffee62e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from icecream import ic\n",
    "\n",
    "from msfm.utils import analysis, parameters\n",
    "from msfm import fiducial_pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b09f37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bdcec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 08:58:51.910426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 08:58:53.730630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38277 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2023-03-02 08:58:53.733067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38277 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2023-03-02 08:58:53.734216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38277 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2023-03-02 08:58:53.735325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38277 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "# strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])\n",
    "# strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "print(strategy.num_replicas_in_sync)\n",
    "\n",
    "n_gpus = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d37123",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b15243f-9854-4321-854d-e71a98f57e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-02 08:58:54  analysis.py INF   Loaded the config \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_batch_size: 4\n",
      "ic| local_batch_size: 1\n"
     ]
    }
   ],
   "source": [
    "conf = analysis.load_config()\n",
    "\n",
    "tfr_pattern = \"/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_???.tfrecord\"\n",
    "\n",
    "params = [\"Om\"]\n",
    "pert_labels = parameters.get_fiducial_perturbation_labels(params)\n",
    "n_perts = len(pert_labels)\n",
    "\n",
    "global_batch_size = n_gpus\n",
    "local_batch_size = global_batch_size // n_gpus\n",
    "\n",
    "ic(global_batch_size)\n",
    "ic(local_batch_size)\n",
    "\n",
    "n_readers = 1\n",
    "n_prefetch = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0fff6-ad01-4a55-a2e5-97f3a237fe38",
   "metadata": {},
   "source": [
    "# Every example is seen exactly once (in eval mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31618591-8bb1-4c65-bc85-e5f16ad064dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "n_runs = 200\n",
    "n_patches = 4\n",
    "n_examples = n_runs * n_patches\n",
    "\n",
    "print(n_examples/global_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13d67d-6a4c-47bf-a281-430d9a44c26e",
   "metadata": {},
   "source": [
    "## not distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54b34d2-cdf0-43fc-87c2-fa780dd09218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-02 08:58:54 fiducial_pip INF   Starting to generate the fiducial training set for i_noise = 0 \n",
      "23-03-02 08:58:54  analysis.py INF   Loaded the config \n",
      "23-03-02 08:58:54  analysis.py INF   Loaded the pixel file \n",
      "23-03-02 08:58:54  analysis.py INF   Loaded the config \n",
      "23-03-02 08:58:54  analysis.py INF   Loaded the pixel file \n",
      "\u001b[1m\u001b[93m23-03-02 08:58:55 tfrecords.py WAR   Tracing parse_inverse_fiducial \u001b[0m\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x149b461e4520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x149b461e4520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "23-03-02 08:58:56  analysis.py INF   Loaded the config \n",
      "\u001b[1m\u001b[93m23-03-02 08:58:56 fiducial_pip WAR   Tracing dset_add_bias \u001b[0m\n",
      "\u001b[1m\u001b[93m23-03-02 08:58:56 fiducial_pip WAR   Tracing dset_add_noise \u001b[0m\n",
      "23-03-02 08:58:56 fiducial_pip INF   Batching into 4 elements locally \n",
      "\u001b[1m\u001b[93m23-03-02 08:58:56 fiducial_pip WAR   Tracing dset_concat_perts \u001b[0m\n",
      "23-03-02 08:58:56 fiducial_pip INF   Successfully generated the fiducial training set with element_spec (TensorSpec(shape=(12, 463872, 4), dtype=tf.float32, name=None), (TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(4,), dtype=tf.int32, name=None))) for i_noise = 0 \n"
     ]
    }
   ],
   "source": [
    "# this should contain 800/global_batch_size batches\n",
    "dset = fiducial_pipeline.get_fiducial_dset(\n",
    "    tfr_pattern,\n",
    "    pert_labels,\n",
    "    global_batch_size,\n",
    "    conf=None,\n",
    "    i_noise=0,\n",
    "    n_readers=n_readers,\n",
    "    n_prefetch=n_prefetch,\n",
    "    is_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234f9a90-6749-4121-bb05-6a1fb60b3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "\n",
    "i = 0\n",
    "for data_vectors, index in dset:\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    \n",
    "    indices.append(index[0])\n",
    "    \n",
    "index_tensor = tf.concat(indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367758fb-e944-464f-9776-ff5f251b45be",
   "metadata": {},
   "source": [
    "### every index is included exactly once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4991931-e5a7-42f9-8dad-58de7a58f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "index_array = index_tensor.numpy()\n",
    "\n",
    "vals, counts = np.unique(index_array, return_counts=True)\n",
    "# should be 800\n",
    "print(len(vals))\n",
    "# should only contain 1\n",
    "print(np.unique(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b457e6b5-121d-4765-a075-24fb0bee5894",
   "metadata": {},
   "source": [
    "## distributed with strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5132479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-02 08:59:26 fiducial_pip INF   Starting to generate the fiducial training set for i_noise = 0 \n",
      "23-03-02 08:59:26  analysis.py INF   Loaded the config \n",
      "23-03-02 08:59:26  analysis.py INF   Loaded the pixel file \n",
      "23-03-02 08:59:27  analysis.py INF   Loaded the config \n",
      "23-03-02 08:59:27  analysis.py INF   Loaded the pixel file \n",
      "23-03-02 08:59:27 fiducial_pip INF   Sharding the dataset according to the input_context \n",
      "\u001b[1m\u001b[93m23-03-02 08:59:27 tfrecords.py WAR   Tracing parse_inverse_fiducial \u001b[0m\n",
      "23-03-02 08:59:27  analysis.py INF   Loaded the config \n",
      "\u001b[1m\u001b[93m23-03-02 08:59:27 fiducial_pip WAR   Tracing dset_add_bias \u001b[0m\n",
      "\u001b[1m\u001b[93m23-03-02 08:59:27 fiducial_pip WAR   Tracing dset_add_noise \u001b[0m\n",
      "23-03-02 08:59:27 fiducial_pip INF   Batching into 1 elements locally \n",
      "\u001b[1m\u001b[93m23-03-02 08:59:27 fiducial_pip WAR   Tracing dset_concat_perts \u001b[0m\n",
      "23-03-02 08:59:27 fiducial_pip INF   Successfully generated the fiducial training set with element_spec (TensorSpec(shape=(3, 463872, 4), dtype=tf.float32, name=None), (TensorSpec(shape=(1,), dtype=tf.int64, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))) for i_noise = 0 \n"
     ]
    }
   ],
   "source": [
    "# don't shuffle\n",
    "# don't repeat\n",
    "def dataset_fn(input_context):\n",
    "    dset = fiducial_pipeline.get_fiducial_dset(\n",
    "        tfr_pattern,\n",
    "        pert_labels,\n",
    "        local_batch_size,\n",
    "        conf=None,\n",
    "        i_noise=0,\n",
    "        n_readers=n_readers,\n",
    "        n_prefetch=n_prefetch,\n",
    "        input_context=input_context,\n",
    "        is_eval=True\n",
    "    )\n",
    "\n",
    "    return dset\n",
    "\n",
    "dist_dset = strategy.distribute_datasets_from_function(dataset_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de94859a-24f8-4d58-9cca-8009eef6a780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "INFO:tensorflow:Gather to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "10\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "20\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "30\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "40\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "50\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "60\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "70\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "80\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "90\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "100\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "110\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "120\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "130\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "140\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "150\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "160\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "170\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "180\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "190\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n",
      "WARNING:tensorflow:gather/all_gather with NCCL or HierarchicalCopy is not supported. Falling back to gather on one device and then broadcast. We're working on a more efficient implementation.\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "\n",
    "i = 0\n",
    "for data_vectors, index in dist_dset:\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    \n",
    "    indices.append(strategy.gather(index[0], axis=0))\n",
    "    \n",
    "index_tensor = tf.stack(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9232e9e-6ad1-4cf7-9e38-8ce94719cd17",
   "metadata": {},
   "source": [
    "### every index is included exactly once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c02f545-0df5-408c-b2b8-5063b1ba1988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "index_array = index_tensor.numpy().ravel()\n",
    "\n",
    "vals, counts = np.unique(index_array, return_counts=True)\n",
    "# should be 800\n",
    "print(len(vals))\n",
    "# should only contain 1\n",
    "print(np.unique(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9e546-9026-45fd-9541-4735ee2f3787",
   "metadata": {},
   "source": [
    "### the different workers are disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468bfc57-c063-471a-8a6d-e6558e4c5d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "index_array = index_tensor.numpy()\n",
    "\n",
    "for i in range(index_array.shape[1]):\n",
    "    exclude_index = i\n",
    "    bool_index = np.arange(n_gpus) != exclude_index\n",
    "\n",
    "    # intersection between the ith worker and all of the rest\n",
    "    print(np.intersect1d(index_array[:,i], index_array[:,bool_index].ravel()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648118e-b902-46aa-afb9-e461a59ee467",
   "metadata": {
    "tags": []
   },
   "source": [
    "# simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5834b1-76ed-4d7c-8d90-6d4aa4c72569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44a3645-2180-4d22-9097-94ec7094ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_000.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_001.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_002.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_003.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_004.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_005.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_006.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_007.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_008.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_009.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_010.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_011.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_012.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_013.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_014.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_015.tfrecord', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# this is at the start of the real pipeline\n",
    "tfr_pattern = \"/pscratch/sd/a/athomsen/DESY3/v2/fiducial/DESy3_fiducial_???.tfrecord\"\n",
    "dset = tf.data.Dataset.list_files(tfr_pattern, shuffle=False)\n",
    "\n",
    "for x in dset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d365485-d918-443e-82c3-e6427bd5611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| n_replicas: 4\n",
      "ic| global_batch_size: 8\n",
      "ic| local_batch_size: 2\n"
     ]
    }
   ],
   "source": [
    "# this can be run separately from above\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "n_replicas = mirrored_strategy.num_replicas_in_sync\n",
    "\n",
    "global_batch_size = 8\n",
    "local_batch_size = global_batch_size//n_replicas\n",
    "\n",
    "ic(n_replicas)\n",
    "ic(global_batch_size)\n",
    "ic(local_batch_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c939de3-9eaf-4e45-b564-21792e4f598f",
   "metadata": {},
   "source": [
    "### multiply the number of elements to take by the number of replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b469e9d-1917-4338-b562-cd03381caf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([0 1], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([2 3], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([4 5], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([6 7], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "1\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([8 9], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([10 11], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([12 13], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([14 15], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "2\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([16 17], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([18 19], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([20 21], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([22 23], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "3\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([24 25], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([26 27], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([28 29], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([30 31], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "4\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([32 33], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([34 35], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([36 37], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([38 39], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_steps = 5\n",
    "\n",
    "def dataset_fn(input_context):\n",
    "    dataset = tf.data.Dataset.range(64)\n",
    "    dataset = dataset.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "    dataset = dataset.batch(local_batch_size)\n",
    "    \n",
    "    dataset = dataset.take(n_replicas*n_steps)\n",
    "    return dataset\n",
    "\n",
    "dist_dataset = mirrored_strategy.distribute_datasets_from_function(dataset_fn)\n",
    "\n",
    "i = 0\n",
    "for x in dist_dataset:\n",
    "    print(i)\n",
    "    print(x, \"\\n\")\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ac8a1-2d0e-4363-8aa7-e498bba73168",
   "metadata": {},
   "source": [
    "### use input_context.num_input_pipelines instead of input_context.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec6d9e21-2c87-4358-96b8-6fa298ba55c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([0 1], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([2 3], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([4 5], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([6 7], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "1\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([8 9], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([10 11], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([12 13], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([14 15], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "2\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([16 17], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([18 19], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([20 21], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([22 23], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "3\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([24 25], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([26 27], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([28 29], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([30 31], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "4\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([32 33], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([34 35], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([36 37], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([38 39], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "5\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([40 41], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([42 43], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([44 45], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([46 47], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "6\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([48 49], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([50 51], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([52 53], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([54 55], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n",
      "7\n",
      "PerReplica:{\n",
      "  0: tf.Tensor([56 57], shape=(2,), dtype=int64),\n",
      "  1: tf.Tensor([58 59], shape=(2,), dtype=int64),\n",
      "  2: tf.Tensor([60 61], shape=(2,), dtype=int64),\n",
      "  3: tf.Tensor([62 63], shape=(2,), dtype=int64)\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dataset_fn(input_context):\n",
    "    dataset = tf.data.Dataset.range(64)\n",
    "    dataset = dataset.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "    dataset = dataset.batch(local_batch_size)\n",
    "    return dataset\n",
    "\n",
    "dist_dataset = mirrored_strategy.distribute_datasets_from_function(dataset_fn)\n",
    "\n",
    "i = 0\n",
    "for x in dist_dataset:\n",
    "    print(i)\n",
    "    print(x, \"\\n\")\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84069e7e-93a4-4edc-a4e5-112dba52577a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_lss",
   "language": "python",
   "name": "deep_lss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a491ce236b7462c2d10c5eba11df8cbf31130dea204ba4d12248d0f0c801e5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
