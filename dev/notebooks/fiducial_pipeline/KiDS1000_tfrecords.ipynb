{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to locally examine the fiducial KiDS1000 .tfrecords\n",
    "\n",
    "Based off https://cosmo-gitlab.phys.ethz.ch/jafluri/cosmogrid_kids1000/-/blob/master/kids1000_analysis/input_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/arne/miniforge3/envs/des/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py:342: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.\n",
      "Instructions for updating:\n",
      "`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.\n",
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:20:38.587318: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-06 13:20:38.587451: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from kids1000_analysis import data, constants, input_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_file = \"/Users/arne/data/KiDS1000/fiducial/fiducial_data_000.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:13:21.084671: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['patch_0', 'patch_1', 'patch_10', 'patch_11', 'patch_12', 'patch_13', 'patch_14', 'patch_2', 'patch_3', 'patch_4', 'patch_5', 'patch_6', 'patch_7', 'patch_8', 'patch_9'])\n",
      "(149504, 10)\n"
     ]
    }
   ],
   "source": [
    "dset_signal = tf.data.TFRecordDataset(tfr_file)\n",
    "dset_signal = data.decode_dset(dset_signal, tag=\"patch\", num_tags=15, shapes=(constants.n_pix_padded, 2*constants.n_z_bins))\n",
    "\n",
    "# add noise, remove mean, ...\n",
    "\n",
    "for temp in dset_signal.take(1):\n",
    "    print(type(temp))\n",
    "    print(temp.keys())\n",
    "    print(temp[\"patch_0\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(2242560, 10)\n"
     ]
    }
   ],
   "source": [
    "dset_concat = data.concat_tag(dset_signal, tag=\"patch\", num_tags=15)\n",
    "\n",
    "for temp in dset_concat.take(1):\n",
    "    print(type(temp))\n",
    "    print(temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['patch_0', 'patch_1', 'patch_10', 'patch_11', 'patch_12', 'patch_13', 'patch_14', 'patch_2', 'patch_3', 'patch_4', 'patch_5', 'patch_6', 'patch_7', 'patch_8', 'patch_9'])\n",
      "(3, 149504, 10)\n"
     ]
    }
   ],
   "source": [
    "dset_batched = dset_signal.batch(3)\n",
    "\n",
    "for temp in dset_batched.take(1):\n",
    "    print(type(temp))\n",
    "    print(temp.keys())\n",
    "    print(temp[\"patch_0\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(45, 149504, 10)\n"
     ]
    }
   ],
   "source": [
    "dset_concat = data.concat_tag(dset_batched, tag=\"patch\", num_tags=15)\n",
    "\n",
    "for temp in dset_concat.take(1):\n",
    "    print(type(temp))\n",
    "    print(temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_batch_size = 3\n",
    "tags = [0]\n",
    "for i in range(3):\n",
    "    tags.append(2 * i + 1)\n",
    "    tags.append(2 * i + 2)\n",
    "\n",
    "\n",
    "dset = input_pipeline.get_train_dset(\n",
    "    batch_size=3,\n",
    "    signal_file_patern=\"/Users/arne/data/KiDS1000/fiducial/fiducial_data_000.tfrecord\",\n",
    "    noise_file_patern=\"/Users/arne/data/KiDS1000/fiducial/noise_data_000.tfrecord\",\n",
    "    data_file=\"/Users/arne/data/KiDS1000/KiDS1000_pixel_data.hdf5\",\n",
    "    with_bary=True,\n",
    "    m_bias=1,\n",
    "    c_bias=0,\n",
    "    node_id=0,\n",
    "    n_node=1,\n",
    "    num_readers=1,\n",
    "    noise_scale=1,\n",
    "    tags=tags,\n",
    "    set_for_eval=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(21, 149504, 10)\n",
      "(7, 149504, 10)\n"
     ]
    }
   ],
   "source": [
    "for temp in dset.take(100):\n",
    "    print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dset(batch_size, signal_file_patern, noise_file_patern, data_file, with_bary, remove_mean=True,\n",
    "                   m_bias=True, c_bias=True, noise_scale=None, tags=None, node_id=0, n_node=1, num_readers=12,\n",
    "                   file_name_shuffle=128, file_name_shuffle_seed=11, signal_shuffle_buffer=128,\n",
    "                   signal_shuffle_seed=1111, noise_shuffle_buffer=128, noise_shuffle_seed=111111, set_for_eval=False,\n",
    "                   prefetch=3):\n",
    "    \"\"\"\n",
    "    Builds the training dataset from given file name patters\n",
    "    :param batch_size: local batch size, will be multiplied with the number of deltas for the delta loss\n",
    "    :param signal_file_patern: File pattern to expand to get the signal files\n",
    "    :param noise_file_patern: File pattern to expand to get the noise files\n",
    "    :param data_file: KiDS data file for mask building and so on\n",
    "    :param with_bary: include baryons (changes the number of deltas -> global batch size)\n",
    "    :param remove_mean: remove the mean from all maps prior to adding the biases\n",
    "    :param m_bias: add multiplicative bias\n",
    "    :param c_bias: add additive shear bias\n",
    "    :param noise_scale: factor to multiply on the noise before addition, should be a tf.Variable\n",
    "    :param tags: Tags to take, i.e. what parameter perturbations to return, defaults to all\n",
    "    :param node_id: The id of the current node (for multi GPU training) defaults to a single GPU\n",
    "    :param n_node: Number of node in the multi GPU training, defaults to 1 (single GPU)\n",
    "    :param num_readers: Parallel read-out from the files expanded from the patterns, this is parallel over files not\n",
    "                        samples in the files\n",
    "    :param file_name_shuffle: shuffle buffer for the files names that were expanded\n",
    "    :param file_name_shuffle_seed: seed for the file name shuffle (node_id will be added)\n",
    "    :param signal_shuffle_buffer: Shuffle buffer for the signal samples\n",
    "    :param signal_shuffle_seed: seed for the signal shuffle (node_id will be added)\n",
    "    :param noise_shuffle_buffer: Shuffle buffer for the noise samples\n",
    "    :param noise_shuffle_seed: Seed for the noise shuffle (node_id will be added)\n",
    "    :param set_for_eval: If this is True, then the dataset won't be shuffled repeated such that one can go through it\n",
    "                         deterministically exactly once. If this is an int > 0 it will be used as tf.random.set_seed\n",
    "                         (defaults to 1) to make the bias addition deterministic\n",
    "    :param prefetch: Prefetch some batches into memory, None -> no prefetching\n",
    "    :return: A dataset that returns samples with a given batchsize in the right ordering for the delta loss\n",
    "    \"\"\"\n",
    "    # TODO: Think about caching\n",
    "\n",
    "    # set the seed if necessary\n",
    "    if set_for_eval:\n",
    "        if n_node > 1:\n",
    "            print(f\"WARNING: You are sharding the eval version of this dataset across more than one node ({n_node})! \"\n",
    "                  f\"The eval version of this different of all number of nodes because of the random state of each node. \"\n",
    "                  f\"Be carefeul not to mix different versions!\")\n",
    "        tf.random.set_seed(int(set_for_eval) + node_id)\n",
    "\n",
    "    # we start with the signal, get the file names shard, shuffle and dataset them\n",
    "    dset_signal = tf.data.Dataset.list_files(signal_file_patern, shuffle=False)\n",
    "    dset_signal = dset_signal.shard(n_node, node_id)\n",
    "    if not set_for_eval:\n",
    "        dset_signal = dset_signal.repeat()\n",
    "        dset_signal = dset_signal.shuffle(file_name_shuffle, seed=file_name_shuffle_seed + node_id)\n",
    "\n",
    "    # interleave\n",
    "    if set_for_eval:\n",
    "        dset_signal = dset_signal.interleave(tf.data.TFRecordDataset, cycle_length=num_readers, block_length=1)\n",
    "    else:\n",
    "        dset_signal = dset_signal.interleave(tf.data.TFRecordDataset, cycle_length=num_readers, block_length=1,\n",
    "                                             num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "\n",
    "    # if we have baryons then the number of tags is higher\n",
    "    if with_bary:\n",
    "        num_tags = 19\n",
    "    else:\n",
    "        num_tags = 15\n",
    "\n",
    "    # decode the dset\n",
    "    if tags is None:\n",
    "        tags = num_tags\n",
    "    dset_signal = data.decode_dset(dset_signal, tag=\"patch\", num_tags=tags,\n",
    "                                   shapes=(constants.n_pix_padded, 2 * constants.n_z_bins))\n",
    "\n",
    "    # get the masks and bring it into the right shape\n",
    "    masks = gen_data_masks(data_file=data_file)\n",
    "    masks = np.concatenate([masks, masks], axis=-1)[None, ...]\n",
    "    tf_masks = tf.constant(masks, dtype=tf.float32)\n",
    "\n",
    "    # number of relevant pixels\n",
    "    n_relevant = np.sum(masks, axis=(0, 1))\n",
    "\n",
    "    # remove mean (this should actually always happen) with correct weights\n",
    "    if remove_mean:\n",
    "        dset_signal = dset_signal.map(lambda x: dset_map_remove_mean(x, weights=constants.n_pix_padded / n_relevant))\n",
    "\n",
    "    # add biases\n",
    "    if m_bias or c_bias:\n",
    "        dset_signal = dset_signal.map(lambda x: dset_map_add_biases(x, m_bias=m_bias, c_bias=c_bias))\n",
    "\n",
    "    # shuffle and batch\n",
    "    if not set_for_eval:\n",
    "        dset_signal = dset_signal.shuffle(signal_shuffle_buffer, seed=signal_shuffle_seed + node_id)\n",
    "    dset_signal = dset_signal.batch(batch_size)\n",
    "\n",
    "    # now we deal with the noise, get the file names shard, shuffle and dataset them\n",
    "    dset_noise = tf.data.Dataset.list_files(noise_file_patern, shuffle=False)\n",
    "    dset_noise = dset_noise.shard(n_node, node_id)\n",
    "    if not set_for_eval:\n",
    "        dset_noise = dset_noise.repeat()\n",
    "        dset_noise = dset_noise.shuffle(file_name_shuffle)\n",
    "\n",
    "    # interleave in parallel\n",
    "    if set_for_eval:\n",
    "        dset_noise = dset_noise.interleave(tf.data.TFRecordDataset, cycle_length=num_readers, block_length=1)\n",
    "    else:\n",
    "        dset_noise = dset_noise.interleave(tf.data.TFRecordDataset, cycle_length=num_readers, block_length=1,\n",
    "                                           num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "\n",
    "    # decode the dataset\n",
    "    dset_noise = data.decode_labeled_dset(dset=dset_noise,\n",
    "                                          shapes=[(constants.n_pix_padded, 2 * constants.n_z_bins), (2,)],\n",
    "                                          samples_only=True, return_dict=True)\n",
    "\n",
    "    # remove the mean\n",
    "    if remove_mean:\n",
    "        dset_noise = dset_noise.map(lambda x: dset_map_remove_mean(x, weights=constants.n_pix_padded / n_relevant))\n",
    "\n",
    "    # shuffle and batch\n",
    "    if not set_for_eval:\n",
    "        dset_noise = dset_noise.shuffle(noise_shuffle_buffer, seed=noise_shuffle_seed + node_id)\n",
    "    dset_noise = dset_noise.batch(batch_size)\n",
    "\n",
    "    # marry the datasets\n",
    "    dset_train = tf.data.Dataset.zip((dset_signal, dset_noise))\n",
    "\n",
    "    # function to add noise + scale the noise according to the noise scale\n",
    "    if noise_scale is None:\n",
    "        noise_scale = 1.0\n",
    "\n",
    "    def add_noise(b1, b2):\n",
    "        for key in b1.keys():\n",
    "            b1[key] += noise_scale*b2[\"sample\"]\n",
    "        return b1\n",
    "\n",
    "    # map the noise addtion\n",
    "    dset_train = dset_train.map(add_noise)\n",
    "\n",
    "    # concat the everything with selected tags\n",
    "    dset_train = data.concat_tag(dset_train, tag=\"patch\", num_tags=tags)\n",
    "\n",
    "    # mask again\n",
    "    dset_train = dset_train.map(lambda x: tf.multiply(x, tf_masks))\n",
    "\n",
    "    # prefetch\n",
    "    if prefetch is not None:\n",
    "        dset_train = dset_train.prefetch(prefetch)\n",
    "\n",
    "    return dset_train\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "des",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a491ce236b7462c2d10c5eba11df8cbf31130dea204ba4d12248d0f0c801e5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
